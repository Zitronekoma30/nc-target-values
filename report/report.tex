\documentclass[12pt,fleqn,a4paper]{article}

\usepackage{latexsym}
\usepackage{url}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{a4wide}
\usepackage{marvosym}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym}
\usepackage{graphics,graphicx,color,subfigure}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\documentclass{article}


\textheight 680pt
\textwidth 460pt
\topmargin -40pt
\oddsidemargin 5pt
\evensidemargin 5pt
\parindent 0pt

\pagestyle{fancyplain} \setlength{\headheight}{16pt}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\thepage}]
    {\fancyplain{}{\rightmark}}
\rhead[\fancyplain{}{\leftmark}]
    {\fancyplain{}{\thepage}}
\cfoot{}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}


\begin{document}
\begin{titlepage}%Institution
\vspace{2cm}
\centerline{
\large{Department of Artificial Intelligence and Human Interfaces}}
\vspace{0.2cm}
\centerline{\large{University of Salzburg}}%Title with one or two Lines(More if wanted)
%\hline
\vspace{1cm}

\centerline{\large{PS Natural Computation}}
\centerline{SS 24}
\vspace{1cm}

\centerline{\Large\textbf{Dynamic Optimization of Target Values}}
\vspace{0.3cm}
\centerline{\Large\textbf{in Neural Network Classification}}
\vspace{1cm}

\vspace{0.4cm}%Date
\centerline{\today}
\vspace{5cm}%Authors

%\hline
\vspace{0.2cm}
Project Members:\\
\centerline{Andrassik Leon, 12209906, leon.andrassik@stud.plus.ac.at}\\
\centerline{Bhuiyan Amreen, 12203597, amreen.bhuiyan@stud.plus.ac.at}\\
\centerline{Yakar Bet\"ul, 12205751, betuel.yakar@stud.plus.ac.at}\\
\centerline{Zauner Sofia, 12118492, sofia.zauner@stud.plus.ac.at}\\
\vspace {1cm}\\

Academic Supervisor: \\
\centerline{Helmut MAYER}
\centerline{helmut@cosy.sbg.ac.at}
\vspace{1.5cm}\\
Correspondence to: \\
\centerline{Universit\"{a}t Salzburg} \\
\centerline{Fachbereich AIHI} \\
\centerline{Jakob--Haringer--Stra\ss e 2} \\
\centerline{A--5020 Salzburg} \\
\centerline{Austria}
\clearpage
\end{titlepage}

%Table of Content
% \setcounter{page}{1}
% \pagenumbering{Roman} %I,II,III... in the TOC
% \tableofcontents

\clearpage
\pagestyle{headings}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}

\abstract{The aim of our project is to implement an alternative method of choosing target values for classification with neural networks.  Instead of using traditional one-hot encoding, which uses fixed values of 0 and 1, this method involves assigning custom target values to each class. 

These target values are dynamically optimized during the training process, which distinguishes our approach from conventional fixed-target training. Our idea is to capture the model's natural predictions and use these to refine the target values.

This approach requires a new rule for determining the predicted class: rather than choosing the neuron with the highest value, we select the neuron whose output value is closest to its corresponding target value and therefore has the lowest error; a method we will refer to as "minimum distance classification".

The goal of our project is to increase training efficiency while maintaining acceptable classification accuracy. To evaluate the performance of our approach, we will test it on the MNIST dataset and compare it with the one-hot encoding method.}

\vspace{4em}

\section{Introduction}
Artificial neural networks (ANNs) have become essential tools in modern classification tasks, mapping complex input data to discrete predefined output categories. During training, the error is calculated as the difference between the desired output and the actual output produced by the network. The objective of the training process is to minimize this error through iterative adjustments of the network's weights and biases. Following the training phase, performance is evaluated using a test set that contains previously unseen data, providing a neutral measure of its generalizability and classification accuracy. \\

Traditionally, the learning process of such networks is based on one-hot encoded target values, where the correct class is represented by the value 1, and all others are assigned a 0. This binary approach became standard due to its simplicity and compatibility with widely used loss functions. However, enforcing these extreme target values could potentially lead to challenges depending on the system's underlying activation function. It may negatively affect gradient flow, reduce training efficiency, and encourage overconfident predictions. \\

One-hot encoding enforces a "winner-takes-all" (WTA) dynamic, where only the neuron with the highest output is considered correct. Since the target vector consists of a 1 for the correct class and a 0 everywhere else, this is effectively equivalent to selecting the output that is closest to its respective target value. Thus, WTA can be seen as a special case of a more general strategy, which is known as nearest target or minimum distance classification. Here we make a general comparison between the entire output distribution given by the network and the available target vectors for each class. Whichever class's target the output is closest to (using a distance metric such as Euclidean distance) will be chosen as the correct class. \\

In this project, we propose a more flexible approach for target value assignment in classification networks. Instead of defining fixed values of 0 and 1, we generate target vectors composed of custom class and non-class values. Here, class values refer to the target values assigned to the positions which would ordinarily be filled with 1 in a one-hot vector, while the non-class values fill all other positions. These alternative targets are not chosen arbitrarily, but rather inferred from the model's own inherent output tendencies during training. By adapting target values dynamically in response to the network's evolving behavior, we aim to guide learning in a way that is more aligned with the model's natural predictions. \\

This change also calls for a rethinking of how predictions are interpreted. Instead of relying on the activation of the highest neuron to determine the predicted class as in WTA, we introduce a "nearest target classification" approach. We select the class whose output vector value is closest to the corresponding target vector, thereby minimizing the error. This nearest target classification strategy reflects the idea that a well-trained model should not necessarily produce a maximal value in any particular output neuron, but rather accurately match the overall desired response. \\

Our hypothesis is that this method improves the training efficiency, while maintaining the classification accuracy. We empirically evaluate this method using the MNIST dataset, comparing its performance against the traditional one-hot encoding approach. \\

\section{Classification with Artificial Neural Networks} % Grundkonzept erkl√§ren, was findet ein classification ANN (linearly seperating hyperplane), wie findet es diese? Evtl. ein Bild

\section{Methodology}
In this section, we describe our approach to implementing dynamic target value optimization for neural network classification. First, we break down the concrete architecture of the target vectors used for training and classification. We then discuss how the class and non-class values that make up these target vectors are initially constructed and subsequently optimized dynamically during training.

\subsection{Target Value Architecture}

To establish our methodology clearly, we first review the traditional one-hot encoding approach before defining our alternative target value structure.

\subsubsection{Traditional One-Hot Encoding}
In standard classification with $C$ classes, the target vector $\mathbf{t}^{(i)}$ for a sample belonging to class $i$ is defined as:

$$\mathbf{t}^{(i)} = [0, 0, \ldots, 0, \underbrace{1}_{i\text{-th position}}, 0, \ldots, 0]$$

For example, with 3 classes, the target vectors are:
$$\mathbf{t}^{(1)} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
\mathbf{t}^{(2)} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad 
\mathbf{t}^{(3)} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$

\subsubsection{Dynamic Target Value Structure}
Our approach replaces the fixed values 0 and 1 with adaptive class and non-class values. For a classification problem with $C$ classes, we define:

\begin{itemize}
\item $c_i$: the class value for class i (replaces 1 in one-hot encoding)
\item $\Bar{c}_i$: the non-class value for class i (replaces 0 in one-hot encoding)
\end{itemize}

The target vector $\mathbf{t}^{(i)}$ for class $i$ becomes:
$$\mathbf{t}^{(i)} = [\Bar{c}_i, \Bar{c}_i, \ldots, \Bar{c}_i, \underbrace{c_i}_{i\text{-th position}}, \Bar{c}_i, \ldots, \Bar{c}_i]$$

Using the same 3-class example:
$$\mathbf{t}^{(1)} = \begin{bmatrix} c_1 \\ \Bar{c}_1 \\ \Bar{c}_1 \end{bmatrix}, \quad 
\mathbf{t}^{(2)} = \begin{bmatrix} \Bar{c}_2 \\ c_2 \\ \Bar{c}_2 \end{bmatrix}, \quad 
\mathbf{t}^{(3)} = \begin{bmatrix} \Bar{c}_3 \\ \Bar{c}_3 \\ c_3 \end{bmatrix}$$

Importantly, in our implementation:
\begin{itemize}
\item Each class has its own class and non-class values $c_i$ and $\Bar{c}_i$
\item All non-class values within a class are the same
\item All non-class values $\Bar{c}_i$ (and in special cases class values) are optimized dynamically during training
\end{itemize}

\subsection{Target Value Initialization and Optimization}
Since a sample's classification depends on finding the nearest target vector to the network's output, we need target vectors that are reasonably well-separated from one another. Given our construction method, this separation can be achieved by ensuring that each class's class and non-class values are sufficiently spaced apart.

Our approach accomplishes this through two stages: first, we select initial class and non-class values based on the network's natural output tendencies, and second, we dynamically increase the spacing between these values during training. This section describes both processes.

\subsection{Initial Target Value Selection}
The key challenge in selecting the initial class and non-class values is balancing two competing requirements: the values should reflect the model's innate output tendencies rather than being chosen arbitrarily, while also providing a distribution that enables sufficient separation, either initially or through the subsequent optimization process.

We explored several strategies to achieve this balance:
\begin{enumerate}
    \item Uniform distribution of class and non-class values
    \item Traditional soft targets (e.g.: $0.8$ and $0.2$)
    \item Mean of untrained-network predictions
    \item Remapping the untrained output means to use the full $(0,1)$ range
\end{enumerate}

After experimenting with these approaches we discovered that these largely lead to predictions which are no better than random guessing, with the notable exception of the second approach, which we will cover more in the results section. Through further discussion and experimentation, we settled on an altered version of the third approach. 

\vspace{2em}
This approach closely resembled the third approach, but substitutes it by slightly nudging the resulting class and non-class values to increase the initial separation. 

\begin{algorithm}
\caption{Adaptive Target Value Computation}
\label{alg:adaptive_threshold}
\begin{algorithmic}[1]
\REQUIRE Network $\mathcal{N}$, test data $\mathcal{D}_{test}$, spacing function, nudge value $\epsilon$
\ENSURE Adjusted non-class and class threshold values for each class

\STATE \textbf{// Compute mean activations for each class}
\FOR{each $(data, target)$ in $\mathcal{D}_{test}$}
    \STATE $output \leftarrow \mathcal{N}(data)$
    \STATE Group outputs by class: add to $outputs[target]$
\ENDFOR

\STATE Extract class and non-class activation values: $(nc\_vals, c\_vals) \leftarrow$ partition$(outputs)$

\FOR{each class $c$}
    \STATE $c\_means[c] \leftarrow$ mean$(c\_vals[c])$
    \STATE $nc\_means[c] \leftarrow$ mean$(nc\_vals[c])$
\ENDFOR

\STATE \textbf{// Apply spacing function to determine initial thresholds}
\STATE $(non\_class\_values, class\_values) \leftarrow$ spacing$(c\_means, nc\_means)$

\STATE \textbf{// Apply nudge adjustments for better separation}

\FOR{each class $c$}
    \IF{$non\_class\_values[c] > class\_values[c]$}
        \STATE $non\_class\_values[c] \leftarrow non\_class\_values[c] + \epsilon$
    \ELSE
        \STATE $non\_class\_values[c] \leftarrow non\_class\_values[c] - \epsilon$
    \ENDIF
    
    \STATE \textbf{// Enforce bounds [0,1] with compensatory adjustments}
    \IF{$non\_class\_values[c] \leq 0$}
        \STATE $non\_class\_values[c] \leftarrow 0$
        \STATE $class\_values[c] \leftarrow \min(\epsilon, 1)$
    \ELSIF{$non\_class\_values[c] \geq 1$}
        \STATE $non\_class\_values[c] \leftarrow 1$
        \STATE $class\_values[c] \leftarrow \max(1 - \epsilon, 0)$
    \ENDIF
\ENDFOR

\RETURN $(non\_class\_values, class\_values)$
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:adaptive_threshold} begins by running a full test epoch through the network and storing the activation outputs grouped by class. For each class $i$, the mean activation in that class's designated output neuron across all samples of class $i$ becomes the class value $c_i$, while the non-class value $\Bar{c}_i$ is computed as the mean activation across all other output neurons for samples of that same class.

This process typically results in densely packed threshold values that provide insufficient separation for effective classification. To address this, the method first applies the chosen spacing function (detailed in the next section) to establish initial thresholds, then further separates the class and non-class values by applying small nudge adjustments of magnitude $\epsilon$. The nudging direction depends on the relative positioning of the values: if non-class values exceed class values, we increase the non-class threshold; otherwise, we decrease it. Boundary enforcement ensures that all values remain within $(0,1)$ while maintaining meaningful separation.

\subsection{Dynamic Target Optimization Algorithm}
While the target values created by the algorithm \ref{alg:adaptive_threshold} are, according to our criteria, based on the innate tendencies of the models and are somewhat well separated there are two issues with the targets generated by this approach.
\begin{enumerate}
    \item As the network learns, the target values will quickly lose any relation to the networks innate output tendencies.
    \item The value's separation depends greatly on the chosen $\epsilon$. Small values of $\epsilon$ result in target values that rarely exceed a range of $(0, 0.3)$, likely because the underlying network outputs are compressed by the softmax function before averaging.
\end{enumerate}
To address these limitations we continuously adapt these generated target values as training progresses, using a method we have come to call "sigma spacing". 

Algorithm~\ref{alg:sigma_spacing} implements this approach by replacing the fixed nudge parameter $\epsilon$ with a dynamic separation criterion based on the standard deviation of current network outputs.

Rather than relying on static target values computed from the initial untrained network, sigma spacing recomputes separation requirements after each training epoch using $\sigma_{sum}$, the sum of standard deviations across current outputs. This is meant to ensure that target values remain relevant as the network's behavior evolves during training. Additionally, by scaling adjustments according to the actual variability in network outputs, the method automatically adapts to the compressed range typical of softmax-normalized activations.

\begin{algorithm}[H]
\caption{Standard Deviation-Based Threshold Adjustment}
\label{alg:sigma_spacing}
\begin{algorithmic}[1]
\REQUIRE Class values $class\_values$, non-class values $non\_class\_values$, scaling factor $\lambda$ (default = 1)
\ENSURE Adjusted threshold values with minimum separation guarantee

\STATE \textbf{// Compute variability measure}
\STATE $\sigma_{sum} \leftarrow$ sum of standard deviations across all current outputs

\FOR{each class $c$}
    \STATE \textbf{// Check separation requirement}
    \IF{$|class\_values[c] - non\_class\_values[c]| < \sigma_{sum}$}
        \STATE \textbf{// Adjust non-class value based on relative positioning}
        \IF{$non\_class\_values[c] \geq class\_values[c]$}
            \STATE $non\_class\_values[c] \leftarrow non\_class\_values[c] + \lambda \sigma_{sum}$
        \ELSE
            \STATE $non\_class\_values[c] \leftarrow non\_class\_values[c] - \lambda \sigma_{sum}$
        \ENDIF
        
        \STATE \textbf{// Handle boundary violations with compensation}
        \IF{$non\_class\_values[c] > 1$}
            \STATE $excess \leftarrow non\_class\_values[c] - 1$
            \STATE $non\_class\_values[c] \leftarrow 1$
            \STATE $class\_values[c] \leftarrow \max(0, class\_values[c] - excess)$
        \ELSIF{$non\_class\_values[c] < 0$}
            \STATE $deficit \leftarrow |non\_class\_values[c]|$
            \STATE $non\_class\_values[c] \leftarrow 0$
            \STATE $class\_values[c] \leftarrow \min(1, class\_values[c] + deficit)$
        \ENDIF
    \ENDIF
    
    \STATE \textbf{// Final bounds enforcement}
    \STATE $class\_values[c] \leftarrow \text{clip}(class\_values[c], 0, 1)$
    \STATE $non\_class\_values[c] \leftarrow \text{clip}(non\_class\_values[c], 0, 1)$
\ENDFOR

\RETURN $(non\_class\_values, class\_values)$
\end{algorithmic}
\end{algorithm}




% Note: Mention unidirectional and bidirectional sigma...

\section{Experimental Setup}
 Our experimental setup is designed to evaluate the performance of our dynamic target value optimization method against traditional one-hot encoding and soft-target baselines. All experiments are conducted on the MNIST dataset.

 \subsection{Dataset and Preprocessing}
 We used the standard MNIST dataset, which consists of 60,000 training images and 10,000 test images of handwritten digits (0-9). The images are normalized using a mean of 0.1307 and a standard deviation of 0.3081, which are the standard values for this dataset.

 \subsection{Network Architecture}
All experiments use the same Convolutional Neural Network (CNN), implemented in PyTorch.

 \begin{itemize}
     \item An initial convolutional layer with 1 input channel, 10 output channels, and a kernel size of 5.
     \item A second convolutional layer with 10 input channels, 20 output channels, and a kernel size of 5. A dropout layer is applied after this convolution.
     \item A fully connected layer with 320 input features and 50 output features.
     \item A final fully connected layer with 50 input features and 10 output features, corresponding to the 10 digit classes.
 \end{itemize}
 The network uses Sigmoid activation functions after the convolutional and first fully connected layers, and a log-softmax activation function on the output layer.

 \subsection{Training Parameters}
 The training process is configured with the following hyperparameters:
 \begin{itemize}
     \item \textbf{Epochs:} The number of training epochs is configurable, but typically set to 4 for our experiments.
     \item \textbf{Batch Size:} We use a batch size of 64 for training and 1000 for testing.
     \item \textbf{Optimizer:} Stochastic Gradient Descent (SGD) is used as the optimizer, with a learning rate of 0.01 and momentum of 0.5.
     \item \textbf{Loss Function:} The loss is calculated using the Negative Log-Likelihood Loss (NLLLoss) between the network's log-softmax output and the target vectors.
 \end{itemize}

 \subsection{Target Value Strategies}
 We compare three different target value strategies:
 \begin{enumerate}
     \item \textbf{One-Hot Encoding:} The traditional approach where the target vector for the correct class contains a 1 at the corresponding index and 0s elsewhere.
     \item \textbf{Soft Targets:} A variation where the target for the correct class is set to 0.8 and the targets for all other classes are set to 0.2.
     \item \textbf{Dynamic Targets:} Our proposed method, where the class and non-class target values are initialized and then dynamically adjusted during training using the "sigma spacing" adaptation method. The initial values are determined by the mean activations of the untrained network on the test set, with a small "nudge" of 0.2 applied for initial separation.
 \end{enumerate}

 \subsection{Evaluation Metrics}
 The performance of each strategy is evaluated based on the following metrics, which are logged using Weights \& Biases:
 \begin{itemize}
     \item \textbf{Hard Accuracy:} The percentage of correctly classified images. For the dynamic target method, classification is determined by the nearest target vector, not the maximum output value.
     \item \textbf{Test Loss:} The average NLLLoss on the test set.
     \item \textbf{Confidence:} The average cosine similarity between the network's output and the target vector of the predicted class.
     \item \textbf{Target Value Separation:} For the dynamic target method, we track the absolute difference between the class and non-class target values for each class, as well as the average, minimum, and maximum separation across all classes.
 \end{itemize}

\section{Results and Analysis}

\section{Discussion and Potential Next Steps}

\section{Conclusion}

\newpage

\section{Milestones} % NOT in final paper
\begin{itemize}
\item {\textbf{26.03.25 - Project Kickoff and Theoretical Outline} \\ Create a rough outline of the project's content based on the given paper. Include loose definitions and a short summary of the introduced methods.}

\item{\textbf{09.04.25 - Goal Specification} \\ Clarify the project idea, define concrete project goals, and deepen the understanding of the topic. Focus especially on precise definitions of target values, class values, and non-class values. Write the abstract and milestone plan for the semester.}

\item{\textbf{23.04.35 - Algorithm Design and Planning} \\ Start working on the paper. Deepen research on alternative target value methods for classification with neural networks. Select and define an approach for alternative target encoding. Plan implementation steps for generating ``class-'' and ``non-class'' values.}

\item{\textbf{07.05.25 - Initial Implementation and Testing of target value algorithm} \\ Initial implementation of the new target encoding method.  Before testing, allocate time for experimental design to ensure solid and interpretable results. Conduct simple experiments using simplified datasets to validate functionality.}

\item{\textbf{21.05.25 - Compelete Implementation and Evaluate on MNIST-Experiments} \\ Complete implementation and testing of the alternative target value methods. Prepare and conduct experiments on the MNIST dataset. Analyze model predictions to evaluate performance compared to traditional one-hot encoding. Discuss results and possible improvements.}

\item{\textbf{04.06.25 - Optimization and Comparative Analysis} \\ Complete improvements and experiments. Analyze experimental results and compare training efficiency and classification accuracy with traditional methods. Document findings and prepare initial draft of project report.}

\item{\textbf{18.06.25 - Finalization} \\ Final preparations for submission. Complete report and presentation.}

\end{itemize}

\newpage

\section{Progress of Work} % NOT in final paper

\subsection{Week 1, Tuesday, 26.03.2025}

\subsection{Week 2, Tuesday, 02.04.2025}

\subsection{Week 3, Tuesday, 09.04.2025}

\subsection{Week 4, Tuesday, 16.04.2025}

\subsection{Week 5, Tuesday, 23.04.2025}

\subsection{Week 6, Tuesday, 30.04.2025}

\subsection{Week 7, Tuesday, 07.05.2025}

\subsection{Week 8, Tuesday, 14.05.2025}

\subsection{Week 9, Tuesday, 21.05.2025}

\subsection{Week 10, Tuesday, 28.05.2025}

\subsection{Week 11, Tuesday, 04.06.2025}

\subsection{Week 12, Tuesday, 11.06.2025}

\subsection{Week 13, Tuesday, 18.06.2025}

\newpage

\section{Subproject Responsibilities} % NOT in final paper

\begin{itemize}
\item{\textbf{Programming of Target Value Calculation Methods (Phyton)} \\
 Leon Andrassik }
\item{\textbf{Research and Preparation of Target Encoding Methods} \\
\indent Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Evaluation and Result Analysis} \\
\indent Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Project Documentation and Preparation of Presentations} \\
\indent Leon Andrassik, Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Progress Presentations} \\
\indent Leon Andrassik, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Final Presentation} \\
\indent Amreen Bhuyian }
\end{itemize}

\newpage

% links go here, NOT in references
\section{Links}

\begin{itemize}
\item Project Page: \url{http://student.cosy.sbg.ac.at/???}
\item PS Page:
\url{http://www.cosy.sbg.ac.at/~helmut/Teaching/PSRobotik/}

\end{itemize}

% real scientific references
\bibliography{}		% .bib files here

\end{document}