\documentclass[12pt,fleqn,a4paper]{article}

\usepackage{latexsym}
\usepackage{url}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{a4wide}
\usepackage{marvosym}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym}
\usepackage{graphics,graphicx,color,subfigure}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\textheight 680pt
\textwidth 460pt
\topmargin -40pt
\oddsidemargin 5pt
\evensidemargin 5pt
\parindent 0pt

\pagestyle{fancyplain} \setlength{\headheight}{16pt}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\thepage}]
    {\fancyplain{}{\rightmark}}
\rhead[\fancyplain{}{\leftmark}]
    {\fancyplain{}{\thepage}}
\cfoot{}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}


\begin{document}
\begin{titlepage}%Institution
\vspace{2cm}
\centerline{
\large{Department of Artificial Intelligence and Human Interfaces}}
\vspace{0.2cm}
\centerline{\large{University of Salzburg}}%Title with one or two Lines(More if wanted)
%\hline
\vspace{1cm}

\centerline{\large{PS Natural Computation}}
\centerline{SS 24}
\vspace{1cm}

\centerline{\Large{\bf{Optimizing of Target Values}} }%Type of the Document
\vspace{1cm}

\vspace{0.4cm}%Date
\centerline{\today}
\vspace{5cm}%Authors

%\hline
\vspace{0.2cm}
Project Members:\\
\centerline{Andrassik Leon, 12209906, leon.andrassik@stud.plus.ac.at}\\
\centerline{Bhuiyan Amreen, 12203597, amreen.bhuiyan@stud.plus.ac.at}\\
\centerline{Yakar Bet\"ul, 12205751, betuel.yakar@stud.plus.ac.at}\\
\centerline{Zauner Sofia, 12118492, sofia.zauner@stud.plus.ac.at}\\
\vspace {1cm}\\

Academic Supervisor: \\
\centerline{Helmut MAYER}
\centerline{helmut@cosy.sbg.ac.at}
\vspace{1.5cm}\\
Correspondence to: \\
\centerline{Universit\"{a}t Salzburg} \\
\centerline{Fachbereich AIHI} \\
\centerline{Jakob--Haringer--Stra\ss e 2} \\
\centerline{A--5020 Salzburg} \\
\centerline{Austria}
\clearpage
\end{titlepage}

%Table of Content
% \setcounter{page}{1}
% \pagenumbering{Roman} %I,II,III... in the TOC
% \tableofcontents

\clearpage
\pagestyle{headings}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}

\abstract{The aim of our project is to implement an alternative method of choosing target values for classification with neural networks.  Instead of using traditional one-hot encoding, which uses fixed values of 0 and 1, this method involves assigning custom target values to each class. 

These target values are dynamically optimized during the training process, which distinguishes our approach from conventional fixed-target training. Our idea is to capture the model's natural predictions and use these to refine the target values.

This approach requires a new rule for determining the predicted class: rather than choosing the neuron with the highest value, we select the neuron whose output value is closest to its corresponding target value and therefore has the lowest error; a method we will refer to as "minimum distance classification".

The goal of our project is to increase training efficiency while maintaining acceptable classification accuracy. To evaluate the performance of our approach, we will test it on the MNIST dataset and compare it with the one-hot encoding method.}

\vspace{4em}

\section{Introduction}
Artificial neural networks (ANNs) have become essential tools in modern classification tasks, mapping complex input data to discrete predefined output categories. During training, the error is calculated as the difference between the desired output and the actual output produced by the network. The objective of the training process is to minimize this error through iterative adjustments of the network's weights and biases. Following the training phase, performance is evaluated using a test set that contains previously unseen data, providing a neutral measure of its generalizability and classification accuracy. \\

Traditionally, the learning process of such networks is based on one-hot encoded target values, where the correct class is represented by the value 1, and all others are assigned a 0. This binary approach became standard due to its simplicity and compatibility with widely used loss functions. However, enforcing these extreme target values could potentially lead to challenges depending on the system's underlying activation function. It may negatively affect gradient flow, reduce training efficiency, and encourage overconfident predictions.\\

One-hot encoding enforces a "winner-takes-all" (WTA) dynamic, where only the neuron with the highest output is considered correct. Since the target vector consists of a 1 for the correct class and a 0 everywhere else, this is effectively equivalent to selecting the output that is closest to its respective target value. Thus, WTA can be seen as a special case of a more general strategy, which is known as nearest target or minimum distance classification. Here we make a general comparison between the entire output distribution given by the network and the available target vectors for each class. Whichever class's target the output is closest to (using a distance metric such as Euclidean distance) will be chosen as the correct class. \\

In this project, we propose a more flexible approach for target value assignment in classification networks. Instead of defining fixed values of 0 and 1, we generate target vectors composed of custom class and non-class values. Here, class values refer to the target values assigned to the positions which would ordinarily be filled with 1 in a one-hot vector, while the non-class values fill all other positions. These alternative targets are not chosen arbitrarily, but rather inferred from the model's own inherent output tendencies during training. By adapting target values dynamically in response to the network's evolving behavior, we aim to guide learning in a way that is more aligned with the model's natural predictions. \\

This change also calls for a rethinking of how predictions are interpreted. Instead of relying on the activation of the highest neuron to determine the predicted class as in WTA, we introduce a "nearest target classification" approach. We select the class whose output vector value is closest to the corresponding target vector, thereby minimizing the error. This nearest target classification strategy reflects the idea that a well-trained model should not necessarily produce a maximal value in any particular output neuron, but rather accurately match the overall desired response. \\

Our hypothesis is that this method improves the training efficiency, while maintaining the classification accuracy. We empirically evaluate this method using the MNIST dataset, comparing its performance against the traditional one-hot encoding approach. \\


\section{Methodology}
In this section, we describe our approach to implementing dynamic target value optimization for neural network classification. First, we break down the concrete architecture of the target vectors used for training and classification. We then discuss how the class and non-class values that make up these target vectors are initially constructed and subsequently optimized dynamically during training.

\subsection{Target Value Architecture}

To establish our methodology clearly, we first review the traditional one-hot encoding approach before defining our alternative target value structure.

\subsubsection{Traditional One-Hot Encoding}
In standard classification with $C$ classes, the target vector $\mathbf{t}^{(i)}$ for a sample belonging to class $i$ is defined as:

$$\mathbf{t}^{(i)} = [0, 0, \ldots, 0, \underbrace{1}_{i\text{-th position}}, 0, \ldots, 0]$$

For example, with 3 classes, the target vectors are:
$$\mathbf{t}^{(1)} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
\mathbf{t}^{(2)} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \quad 
\mathbf{t}^{(3)} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$

\subsubsection{Dynamic Target Value Structure}
Our approach replaces the fixed values 0 and 1 with adaptive class and non-class values. For a classification problem with $C$ classes, we define:

\begin{itemize}
\item $c_i$: the class value for class i (replaces 1 in one-hot encoding)
\item $\Bar{c}_i$: the non-class value for class i (replaces 0 in one-hot encoding)
\end{itemize}

The target vector $\mathbf{t}^{(i)}$ for class $i$ becomes:
$$\mathbf{t}^{(i)} = [\Bar{c}_i, \Bar{c}_i, \ldots, \Bar{c}_i, \underbrace{c_i}_{i\text{-th position}}, \Bar{c}_i, \ldots, \Bar{c}_i]$$

Using the same 3-class example:
$$\mathbf{t}^{(1)} = \begin{bmatrix} c_1 \\ \Bar{c}_1 \\ \Bar{c}_1 \end{bmatrix}, \quad 
\mathbf{t}^{(2)} = \begin{bmatrix} \Bar{c}_2 \\ c_2 \\ \Bar{c}_2 \end{bmatrix}, \quad 
\mathbf{t}^{(3)} = \begin{bmatrix} \Bar{c}_3 \\ \Bar{c}_3 \\ c_3 \end{bmatrix}$$

Importantly, in our implementation:
\begin{itemize}
\item Each class has its own class and non-class values $c_i$ and $\Bar{c}_i$
\item All non-class values within a class are the same
\item All non-class values $\Bar{c}_i$ (and in special cases class values) are optimized dynamically during training 
\end{itemize}

\subsection{Target Value Initialization and Optimization}
Since a sample's classification depends on finding the nearest target vector to the network's output, we need target vectors that are reasonably well-separated from one another. Given our construction method, this separation can be achieved by ensuring that each class's class and non-class values are sufficiently spaced apart.

Our approach accomplishes this through two stages: first, we select initial class and non-class values based on the network's natural output tendencies, and second, we dynamically increase the spacing between these values during training. This section describes both processes.

\subsection{Initial Target Value Selection}
Note: talk about different warmup methods but mention which one is best and that we will only go into detail on that one

\subsection{Dynamic Target Optimization Algorithm}
Note: Mention uni directional and bi directional sigma

\section{Experimental Setup}

\section{Results and Analysis}

\section{Discussion and Conclusion}

\newpage

\section{Milestones} % NOT in final paper
\begin{itemize}
\item {\textbf{26.03.25 - Project Kickoff and Theoretical Outline} \\ Create a rough outline of the project's content based on the given paper. Include loose definitions and a short summary of the introduced methods.}

\item{\textbf{09.04.25 - Goal Specification} \\ Clarify the project idea, define concrete project goals, and deepen the understanding of the topic. Focus especially on precise definitions of target values, class values, and non-class values. Write the abstract and milestone plan for the semester.}

\item{\textbf{23.04.35 - Algorithm Design and Planning} \\ Start working on the paper. Deepen research on alternative target value methods for classification with neural networks. Select and define an approach for alternative target encoding. Plan implementation steps for generating ``class-'' and ``non-class'' values.}

\item{\textbf{07.05.25 - Initial Implementation and Testing of target value algorithm} \\ Initial implementation of the new target encoding method.  Before testing, allocate time for experimental design to ensure solid and interpretable results. Conduct simple experiments using simplified datasets to validate functionality.}

\item{\textbf{21.05.25 - Compelete Implementation and Evaluate on MNIST-Experiments} \\ Complete implementation and testing of the alternative target value methods. Prepare and conduct experiments on the MNIST dataset. Analyze model predictions to evaluate performance compared to traditional one-hot encoding. Discuss results and possible improvements.}

\item{\textbf{04.06.25 - Optimization and Comparative Analysis} \\ Complete improvements and experiments. Analyze experimental results and compare training efficiency and classification accuracy with traditional methods. Document findings and prepare initial draft of project report.}

\item{\textbf{18.06.25 - Finalization} \\ Final preparations for submission. Complete report and presentation.}

\end{itemize}

\newpage

\section{Progress of Work} % NOT in final paper

\subsection{Week 1, Tuesday, 26.03.2025}

\subsection{Week 2, Tuesday, 02.04.2025}

\subsection{Week 3, Tuesday, 09.04.2025}

\subsection{Week 4, Tuesday, 16.04.2025}

\subsection{Week 5, Tuesday, 23.04.2025}

\subsection{Week 6, Tuesday, 30.04.2025}

\subsection{Week 7, Tuesday, 07.05.2025}

\subsection{Week 8, Tuesday, 14.05.2025}

\subsection{Week 9, Tuesday, 21.05.2025}

\subsection{Week 10, Tuesday, 28.05.2025}

\subsection{Week 11, Tuesday, 04.06.2025}

\subsection{Week 12, Tuesday, 11.06.2025}

\subsection{Week 13, Tuesday, 18.06.2025}

\newpage

\section{Subproject Responsibilities} % NOT in final paper

\begin{itemize}
\item{\textbf{Programming of Target Value Calculation Methods (Phyton)} \\
 Leon Andrassik }
\item{\textbf{Research and Preparation of Target Encoding Methods} \\
\indent Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Evaluation and Result Analysis} \\
\indent Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Project Documentation and Preparation of Presentations} \\
\indent Leon Andrassik, Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Progress Presentations} \\
\indent Leon Andrassik, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Final Presentation} \\
\indent Amreen Bhuyian }
\end{itemize}

\newpage

% links go here, NOT in references
\section{Links}

\begin{itemize}
\item Project Page: \url{http://student.cosy.sbg.ac.at/???}
\item PS Page:
\url{http://www.cosy.sbg.ac.at/~helmut/Teaching/PSRobotik/}

\end{itemize}

% real scientific references
\bibliography{}		% .bib files here

\end{document}
