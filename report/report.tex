\documentclass[12pt,fleqn,a4paper]{article}

\usepackage{latexsym}
\usepackage{url}
\usepackage{xspace}
\usepackage{epsfig}
\usepackage{psfrag}
\usepackage{a4wide}
\usepackage{marvosym}
\usepackage{amsmath,amsfonts,amssymb,amsthm,latexsym}
\usepackage{graphics,graphicx,color,subfigure}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\textheight 680pt
\textwidth 460pt
\topmargin -40pt
\oddsidemargin 5pt
\evensidemargin 5pt
\parindent 0pt

\pagestyle{fancyplain} \setlength{\headheight}{16pt}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\lhead[\fancyplain{}{\thepage}]
    {\fancyplain{}{\rightmark}}
\rhead[\fancyplain{}{\leftmark}]
    {\fancyplain{}{\thepage}}
\cfoot{}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}


\begin{document}
\begin{titlepage}%Institution
\vspace{2cm}
\centerline{
\large{Department of Artificial Intelligence and Human Interfaces}}
\vspace{0.2cm}
\centerline{\large{University of Salzburg}}%Title with one or two Lines(More if wanted)
%\hline
\vspace{1cm}

\centerline{\large{PS Natural Computation}}
\centerline{SS 24}
\vspace{1cm}

\centerline{\Large{\bf{Optimizing of Target Values}} }%Type of the Document
\vspace{1cm}

\vspace{0.4cm}%Date
\centerline{\today}
\vspace{5cm}%Authors

%\hline
\vspace{0.2cm}
Project Members:\\
\centerline{Andrassik Leon, 12209906, leon.andrassik@stud.plus.ac.at}\\
\centerline{Bhuiyan Amreen, 12203597, amreen.bhuiyan@stud.plus.ac.at}\\
\centerline{Yakar Bet\"ul, 12205751, betuel.yakar@stud.plus.ac.at}\\
\centerline{Zauner Sofia, 12118492, sofia.zauner@stud.plus.ac.at}\\
\vspace {1cm}\\

Academic Supervisor: \\
\centerline{Helmut MAYER}
\centerline{helmut@cosy.sbg.ac.at}
\vspace{1.5cm}\\
Correspondence to: \\
\centerline{Universit\"{a}t Salzburg} \\
\centerline{Fachbereich AIHI} \\
\centerline{Jakob--Haringer--Stra\ss e 2} \\
\centerline{A--5020 Salzburg} \\
\centerline{Austria}
\clearpage
\end{titlepage}

%Table of Content
% \setcounter{page}{1}
% \pagenumbering{Roman} %I,II,III... in the TOC
% \tableofcontents

\clearpage
\pagestyle{headings}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}
\pagenumbering{arabic}  %Better if TOC is variable (more than one page)
\setcounter{page}{1}

\abstract{}
The aim of our project is to implement an alternative method of choosing target values for classification with neural networks.  Instead of using traditional one-hot-encoding, which uses fixed values of 0 and 1, this method involves assigning custom target values to each class. 

These target values are optimized dynamically during the training process, which distinguishes our approach from conventional fixed-target training. Our idea is to capture the model's natural predictions and use these to refine the target values.

This approach requires a new rule for determining the predicted class: rather than choosing the neuron with the highest value, we select the neuron whose output value is closest to its corresponding target value and therefore has the lowest error; a method we will refer to as the "minimum error rule".

The goal of our project is to increase training efficiency while maintaining acceptable classification accuracy. To evaluate the performance of our approach we will test it on the MNIST dataset and compare it to the one-hot encoding method.

\newpage

\section{Introduction}
Artificial neural networks (ANNs) have become essential tools in modern classification tasks, mapping complex input data to discrete predefined output categories. The network's error is calculated as the difference between the desired output and the actual output produced by the network. The objective of the training process is to minimize this error through iterative adjustments of the network's weights and biases. Following the training phase, the network's performance is evaluated using a test set containing previously unseen data, which provides a neutral measure of its generalization capability and classification accuracy. \\

Traditionally, the learning process of such networks is based on one-hot encoded target values, where the correct class is represented by the value 1, and all others are assigned a 0. This binary approach became standard due to its simplicity and compatibility with widely used loss functions. However, enforcing these extreme target values possibly leads to challenges, depending on the system's underlying activation function. It can negatively affect gradient flow, reduce training efficiency, and encourage overconfident predictions, making it a suboptimal choice by limiting the learning system's potential.\\

One-hot encoding enforces a "winner-takes-all" dynamic, where only the neuron with the highest output is considered correct. This may oversimplify the refined behavior of a learning system. The process of learning in neural networks is not inherently about selecting the maximum value, but about minimizing error. When every output except the "correct" class is forced to zero, the network is pushed to create large contrasts in activation, which can slow down learning or introduce instability. \\

In this project, we propose a more flexible approach for target value assignment in classification networks. Instead of defining fixed values of 0 and 1, we generate target vectors composed of custom class and non-class values. These alternative targets are not chosen arbitrarily, but rather inferred from the model's own inherent output tendencies during training. By adapting target values dynamically in response to the network's evolving behavior, we aim to guide learning in a way that is more aligned with the model's natural predictions. \\

This change also calls for a rethinking of how predictions are interpreted. Instead of relying on the activation of the highest neuron to determine the predicted class, we introduce the "minimum error rule". We select the output neuron whose value is closest to its corresponding target value, thereby minimizing the error. This "closest-takes-all" strategy reflects the idea that a well-trained model should not necessarily produce a maximal value, but rather accurately match the desired response. \\

Our hypothesis is that this method improves the training efficency, while maintaining, or enhancing, the classification accuracy. We empirically evaluate this method using the MNIST dataset, comparing its performance against the traditional one-hot encoding approach. \\





\newpage

\section{Milestones} % NOT in final paper
\begin{itemize}
\item {\textbf{26.03.25 - Project Kickoff and Theoretical Outline} \\ Create a rough outline of the project's content based on the given paper. Include loose definitions and a short summary of the introduced methods.}

\item{\textbf{09.04.25 - Goal Specification} \\ Clarify the project idea, define concrete project goals, and deepen the understanding of the topic. Focus especially on precise definitions of target values, class values, and non-class values. Write the abstract and milestone plan for the semester.}

\item{\textbf{23.04.35 - Algorithm Design and Planning} \\ Start working on the paper. Deepen research on alternative target value methods for classification with neural networks. Select and define an approach for alternative target encoding. Plan implementation steps for generating ``class-'' and ``non-class'' values.}

\item{\textbf{07.05.25 - Initial Implementation and Testing of target value algorithm} \\ Initial implementation of the new target encoding method.  Before testing, allocate time for experimental design to ensure solid and interpretable results. Conduct simple experiments using simplified datasets to validate functionality.}

\item{\textbf{21.05.25 - Compelete Implementation and Evaluate on MNIST-Experiments} \\ Complete implementation and testing of the alternative target value methods. Prepare and conduct experiments on the MNIST dataset. Analyze model predictions to evaluate performance compared to traditional one-hot encoding. Discuss results and possible improvements.}

\item{\textbf{04.06.25 - Optimization and Comparative Analysis} \\ Complete improvements and experiments. Analyze experimental results and compare training efficiency and classification accuracy with traditional methods. Document findings and prepare initial draft of project report.}

\item{\textbf{18.06.25 - Finalization} \\ Final preparations for submission. Complete report and presentation.}

\end{itemize}

\newpage

\section{Progress of Work} % NOT in final paper

\subsection{Week 1, Tuesday, 26.03.2025}

\subsection{Week 2, Tuesday, 02.04.2025}

\subsection{Week 3, Tuesday, 09.04.2025}

\subsection{Week 4, Tuesday, 16.04.2025}

\subsection{Week 5, Tuesday, 23.04.2025}

\subsection{Week 6, Tuesday, 30.04.2025}

\subsection{Week 7, Tuesday, 07.05.2025}

\subsection{Week 8, Tuesday, 14.05.2025}

\subsection{Week 9, Tuesday, 21.05.2025}

\subsection{Week 10, Tuesday, 28.05.2025}

\subsection{Week 11, Tuesday, 04.06.2025}

\subsection{Week 12, Tuesday, 11.06.2025}

\subsection{Week 13, Tuesday, 18.06.2025}

\newpage

\section{Subproject Responsibilities} % NOT in final paper

\begin{itemize}
\item{\textbf{Programming of Target Value Calculation Methods (Phyton)} \\
 Leon Andrassik }
\item{\textbf{Research and Preparation of Target Encoding Methods} \\
\indent Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Evaluation and Result Analysis} \\
\indent Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner}
\item{\textbf{Project Documentation and Preparation of Presentations} \\
\indent Leon Andrassik, Amreen Bhuiyan, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Progress Presentations} \\
\indent Leon Andrassik, Bet\"ul Yakar, Sofia Zauner }
\item{\textbf{Presenting Final Presentation} \\
\indent Amreen Bhuyian }
\end{itemize}

\newpage

% links go here, NOT in references
\section{Links}

\begin{itemize}
\item Project Page: \url{http://student.cosy.sbg.ac.at/???}
\item PS Page:
\url{http://www.cosy.sbg.ac.at/~helmut/Teaching/PSRobotik/}

\end{itemize}

% real scientific references
\bibliography{}		% .bib files here

\end{document}
